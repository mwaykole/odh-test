apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
    openshift.io/display-name: {{ display_name| default("OVMS-Runtime")  }}  # Jinja parameter
  labels:
    opendatahub.io/dashboard: "true"
  name: {{ runtime_name| default("kserve-ovms")  }}  # Jinja parameter
spec:
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "8888"
  containers:
  - args:
    - --model_name={% raw %}{{ .Name }}{% endraw %}
    - --port=8001
    - --rest_port=8888
    - --model_path=/mnt/models
    - --file_system_poll_wait_seconds=0
    - --grpc_bind_address=0.0.0.0
    - --rest_bind_address=0.0.0.0
    - --target_device=AUTO
    - --metrics_enable
    image:  {{ image| default("quay.io/modh/openvino_model_server@sha256:9086c1ba1ba30d358194c534f0563923aab02d03954e43e9f3647136b44a5daf")  }}   # Jinja parameter
    name: kserve-container
    ports:
    - containerPort: 8888
      protocol: TCP
  multiModel: false
  protocolVersions:
  - v2
  - grpc-v2
  supportedModelFormats:
  - autoSelect: true
    name: openvino_ir
    version: opset13
  - name: onnx
    version: "1"
  - autoSelect: true
    name: tensorflow
    version: "1"
  - autoSelect: true
    name: tensorflow
    version: "2"
  - autoSelect: true
    name: paddle
    version: "2"
  - autoSelect: true
    name: pytorch
    version: "2"